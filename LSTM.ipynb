{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification of spam/ham mails with using **LSTM**\n",
    "## LSTM (Long Short Term Memory)\n",
    "\n",
    "Long Short-Term Memory networks (LSTMs) are a specialized kind of recurrent neural network (RNN) capable of learning long-term dependencies. Below are some of the fundamental concepts underlying LSTMs:\n",
    "\n",
    "### 1. **Memory Cells**\n",
    "- The core component of an LSTM unit is the memory cell, designed to maintain information in memory for long periods. The cell's ability to retain or discard information selectively makes LSTMs suitable for tasks that require remembering or forgetting data over long sequences.\n",
    "\n",
    "### 2. **Gates**\n",
    "LSTMs utilize three distinct types of gates to control the flow of information:\n",
    "   - **Input gate**: Controls how much new information is added to the cell state.\n",
    "   - **Forget gate**: Determines the amount of information thrown away from the current cell state.\n",
    "   - **Output gate**: Decides what information to output based on current input and the memory of the cell.\n",
    "\n",
    "### 3. **Statefulness**\n",
    "- LSTMs maintain two states throughout time steps:\n",
    "   - **Cell state**: Acts as the long-term memory, which carries relevant information throughout the processing of the sequence.\n",
    "   - **Hidden state**: Represents the short-term memory that provides the current output of the LSTM unit at each step.\n",
    "\n",
    "### 4. **Backpropagation Through Time (BPTT)**\n",
    "- LSTMs use a specialized form of backpropagation called Backpropagation Through Time, which is essential for learning temporal dependencies. BPTT involves unfolding the LSTM over time steps and applying traditional backpropagation.\n",
    "\n",
    "### 5. **Advantages Over Standard RNNs**\n",
    "- **Handling Long-Term Dependencies**: Unlike standard RNNs that fail to capture long-term dependencies due to vanishing gradients, LSTMs are designed to address this through their architecture.\n",
    "- **Flexibility**: They can learn which data in a sequence is important to remember or ignore, thus enhancing their efficiency and effectiveness in sequence modeling.\n",
    "\n",
    "### 6. **Challenges and Limitations**\n",
    "- **Computational Complexity**: Due to their sophisticated architecture, LSTMs are more computationally intensive than standard RNNs, leading to longer training times.\n",
    "- **Parameter Tuning**: LSTMs involve numerous hyperparameters that need careful tuning to perform optimally, which can be a complex and time-consuming process.\n",
    "\n",
    "These principles are why LSTMs excel in tasks like speech recognition, text generation, and spam detection, where understanding complex relationships and dependencies over time is crucial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 15:16:43.469064: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-26 15:16:43.650659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 15:16:44.863116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "WARNING:tensorflow:From /tmp/ipykernel_1615/1448305103.py:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 15:16:46.540574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.648414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.648829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.849328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.849659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.849677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-26 15:16:46.850495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-26 15:16:46.850560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 5558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "# 1.14.0\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the dataset is 37685\n",
      "The number of columns in the dataset is 2\n",
      "  label                                               text\n",
      "0  spam  save your money buy getting this thing here yo...\n",
      "1  spam  save your money buy getting this thing here yo...\n",
      "2  spam  wall street phenomenon reaps rewards small cap...\n",
      "3  spam  only our software is guaranteed 100 legal name...\n",
      "4  spam  rely on us for your online prescription orderi...\n"
     ]
    }
   ],
   "source": [
    "# Open the prepared dataset, and split it into training and testing sets\n",
    "data:pd.DataFrame = pd.read_csv(\"./dataset/spameyes_dataset.csv\", lineterminator='\\n')\n",
    "print(f\"The number of rows in the dataset is {data.shape[0]}\")\n",
    "print(f\"The number of columns in the dataset is {data.shape[1]}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                               text\n",
      "0          1  save your money buy getting this thing here yo...\n",
      "1          1  save your money buy getting this thing here yo...\n",
      "2          1  wall street phenomenon reaps rewards small cap...\n",
      "3          1  only our software is guaranteed 100 legal name...\n",
      "4          1  rely on us for your online prescription orderi...\n",
      "...      ...                                                ...\n",
      "37680      1  win $ 300usd and a cruise ! raquel 's casino ,...\n",
      "37681      1  you have been asked to join kiddin the list ow...\n",
      "37682      0  anglicization of composers ' names judging fro...\n",
      "37683      0  re : 6 . 797 , comparative method : n - ary co...\n",
      "37684      0  re : american - english in australia hello ! i...\n",
      "\n",
      "[37685 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1615/4287341439.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"label\"] = data[\"label\"].replace([\"spam\", \"ham\"], [1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Replace the spam and ham labels with 1 and 0 respectively\n",
    "data[\"label\"] = data[\"label\"].replace([\"spam\", \"ham\"], [1, 0])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37685 entries, 0 to 37684\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   37685 non-null  int64 \n",
      " 1   text    37685 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 589.0+ KB\n",
      "None\n",
      "If there is any missing value in the dataset: False\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(f\"If there is any missing value in the dataset: {data.isnull().values.any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of the labels\n",
    "data[\"label\"].value_counts().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the training set is 30148\n",
      "The number of rows in the testing set is 7537\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X:pd.DataFrame = data[\"text\"]\n",
    "y:pd.DataFrame = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0xCAFECAFE)\n",
    "\n",
    "# X_train is for training the model, X_test is for testing the model\n",
    "# y_train is the actual label for the training set, y_test is the actual label for the testing set\n",
    "\n",
    "# Measure how many columns in the training set and testing set\n",
    "print(f\"The number of rows in the training set is {X_train.shape[0]}\")\n",
    "print(f\"The number of rows in the testing set is {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[837, 11, 85, 3717, 16, 2051, 11, 1041, 2, 104, 397, 26, 183, 32, 1929, 258, 7961, 9, 65, 13, 841, 777, 669, 29, 153, 56, 102, 11, 102, 1049, 133, 2, 1979, 907, 1141, 39539], [261, 200, 34, 349, 380, 1224, 593, 399, 1224, 674, 603, 1714, 1623, 3534, 571, 750, 559, 1190, 253, 106, 1353, 1662, 376, 1225, 609, 450, 827, 97781, 1143, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 666, 318, 707, 566, 432, 179, 483, 566, 432, 179, 473, 1300, 10, 804, 13, 5, 1106, 318], [118, 2365, 230, 131, 306, 1768, 5744, 11, 796, 145, 306, 13, 841, 254, 1330, 42, 7016, 54, 66, 17, 23, 5744, 45415, 568, 548, 198, 154, 349, 275, 2, 254, 284, 708, 314, 135, 135, 363, 126, 2365, 230, 131, 306, 1768, 545, 254, 11, 24, 5, 924, 726, 8, 7, 1, 2804, 2876, 134, 2, 521, 5, 1392, 3, 1768, 23, 1, 306, 355, 2365, 230, 131, 306, 13, 1, 180, 11, 1101, 133, 12, 68, 15, 5, 67592, 6650, 9, 80, 706, 29, 306, 71, 137, 145, 2, 61, 3064, 8, 10, 20, 9, 17, 218, 982, 2347, 167, 5744], [203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 203, 57, 98, 206, 9, 6, 10, 3535, 19103, 239, 160, 106, 558, 16191, 21265, 2171, 54443], [1, 920, 1480, 441, 5340, 15651, 31, 5, 149, 106, 24196, 14688, 97782, 4667, 1896, 1902, 2822, 1031, 45416, 45417, 1561, 493, 45418, 568, 374, 915, 401, 1116, 1155, 1276, 141, 1009, 920, 2408, 7, 24, 81, 1005, 21, 52, 3, 1233, 1570, 3, 1, 913, 5340, 15651, 4667, 1896, 219, 2794, 2358, 8, 777, 401, 956, 25, 15, 5, 4056, 12155, 3, 1, 2409, 920, 3, 8536, 4236, 399, 744, 1222, 4, 3304, 52, 908, 177, 1222, 4, 2013, 3397, 54444, 2563, 74, 14, 40, 62, 112, 51, 388, 2, 1, 1116, 142, 59, 168, 366, 55, 493, 579, 1414, 2206, 13465, 10, 858, 9, 5610, 4, 852, 30, 34728, 262, 3, 1, 823, 2805, 26, 6, 1, 913, 5340, 15651, 1896, 9, 30, 10, 158, 39540, 32, 219, 12776, 15651, 3427, 1465, 4, 1381, 2, 7502, 1, 7962, 3, 1, 513, 1208, 142, 3, 766, 6, 32, 898, 523, 16, 723, 10523, 23, 1613, 3427, 4, 2734, 5703, 11915, 2, 35, 1570, 9, 966, 4, 1954, 6, 5, 1403, 64, 185, 5121, 6651, 269, 77, 766, 23, 38, 1412, 318, 3, 78, 431, 77, 1114, 4, 54445, 30, 87, 2723, 11386, 23, 1267, 1, 199, 26, 2119, 38, 2613, 79, 23, 63, 7, 24, 5485, 21, 52, 3, 1, 1624, 1570, 3, 1, 2409, 1414, 920, 2, 404, 14, 4610, 4, 920, 771, 7, 19, 2, 61, 6, 152, 16, 405, 26121, 16817, 920, 976, 10694, 40, 62, 97783, 4611, 22, 624, 1281, 2473, 54446, 7, 19, 2031, 2, 417, 14, 1116, 7963, 1055, 10524, 14, 606, 9, 2202, 4, 14, 101, 3261, 2, 7, 6, 1794, 2856, 7, 2570, 10, 9, 6, 246, 16, 32, 248, 3095, 2, 1056, 1238, 976, 4, 54447, 3, 1, 858, 723, 21, 17, 44, 2564, 6, 1, 505, 742, 35, 335, 1144, 4, 3225, 2, 14, 1009, 920, 606, 10694, 4, 587, 2, 295, 14, 715, 4, 1561, 512, 1276, 435, 359, 23, 5340, 15651, 350, 7, 8, 241, 240, 3, 32, 1702, 158, 418, 45, 28, 348, 2, 10, 62, 112, 152, 14, 606, 10694, 983, 14689, 39541, 22648, 255, 219, 2555]]\n"
     ]
    }
   ],
   "source": [
    "# With using the Keras Tokenizer, we can convert the text data into sequences\n",
    "# These processes are called tokenization and integer encoding\n",
    "tokenizer:Tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded:np.ndarray = tokenizer.texts_to_sequences(X_train)\n",
    "print(X_train_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in the dataset is 210306\n",
      "The first 25 words in the dataset are [('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5), ('in', 6), ('you', 7), ('for', 8), ('is', 9), ('this', 10), ('i', 11), ('that', 12), ('on', 13), ('your', 14), ('be', 15), ('with', 16), ('it', 17), (\"'\", 18), ('are', 19), ('or', 20), ('as', 21), ('com', 22), ('from', 23), ('have', 24), ('will', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Let's check which numbers correspond to which words\n",
    "word_index:dict = tokenizer.word_index\n",
    "print(f\"The number of unique words in the dataset is {len(word_index)}\")\n",
    "print(f\"The first 25 words in the dataset are {list(word_index.items())[:25]}\")\n",
    "\n",
    "# As we can see, the number is assigned to the word based on the frequency of the word.\n",
    "# The lower the number, the more frequent the word is in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the dataset is 210306\n",
      "The number of words that are not so frequent in the dataset is 112526\n",
      "The proportion of the rare words(frequency < 2 in the entire dataset) in the dataset is 53.51%\n"
     ]
    }
   ],
   "source": [
    "# However, there are some words that are not so frequent, less than 2 times in the entire dataset for example.\n",
    "# Let's check how many words are there in the dataset that are not so frequent\n",
    "threshold:int = 2\n",
    "total_words_count:int = len(word_index)\n",
    "rare_words_count:int  = len([word for word, count in tokenizer.word_counts.items() if count < threshold])\n",
    "print(f\"The number of words in the dataset is {total_words_count}\")\n",
    "print(f\"The number of words that are not so frequent in the dataset is {rare_words_count}\")\n",
    "print(f\"The proportion of the rare words(frequency < {threshold} in the entire dataset) in the dataset is {rare_words_count / total_words_count * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single function to train, evalulate, and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Clear the output after each epoch\n",
    "class ClearOutput(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {logs['loss']}, Accuracy: {logs['accuracy']}\")\n",
    "\n",
    "def train_evaluate_lstm_model(data                      :pd.Series, \n",
    "                              labels                    :pd.Series, \n",
    "                              max_length                :int, \n",
    "                              embedding_dimension       :int=32, \n",
    "                              hidden_units              :int=32, \n",
    "                              reduced_vocabulary_size   :int=1000, \n",
    "                              epochs                    :int=20, \n",
    "                              batch_size                :int=64, \n",
    "                              validation_split          :float=0.2, \n",
    "                              patience                  :int=3) -> Sequential:\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=reduced_vocabulary_size)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    X_encoded = tokenizer.texts_to_sequences(data)\n",
    "    X_padded = pad_sequences(X_encoded, maxlen=max_length)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    split_index = int(len(data) * (1 - validation_split))\n",
    "    X_train, X_test = X_padded[:split_index], X_padded[split_index:]\n",
    "    y_train, y_test = labels[:split_index], labels[split_index:]\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=reduced_vocabulary_size + 1, \n",
    "                        output_dim=embedding_dimension, \n",
    "                        input_length=max_length))\n",
    "    model.add(LSTM(units=hidden_units))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))  # Using sigmoid for binary classification\n",
    "    \n",
    "    # Model compilation\n",
    "    model.compile(optimizer=\"rmsprop\", \n",
    "                  loss=\"binary_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=patience, \n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    # Model training\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_split=validation_split, \n",
    "                        callbacks=[early_stopping, ClearOutput()])\n",
    "    \n",
    "    # Model evaluation\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"The loss of the model is {loss:.4f}\")\n",
    "    print(f\"The accuracy of the model is {accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Training and validation loss plot\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying uniformly reducing the word number to reduce the load\n",
    "Some emails are too lengthy(having too many word tokenized vectors). And, the model dimension should be fixed for the biggest email, as the overall model input data size is severely exaggerated(Event for shorter data, we have to pad the sequence to fill the data, resulting in wasting memory space).\n",
    "\n",
    "This approach cuts every email datum's word vector under the threshold(e.g. 300 word vectors per emails), greatly reducing the input data size and boost model training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  837    11    85  3717    16  2051    11  1041     2   104   397    26\n",
      "    183    32  1929   258  7961     9    65    13   841   777   669    29\n",
      "    153    56   102    11   102  1049   133     2  1979   907  1141 39539\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  707   566   432   179   666   318   707   566   432   179   483   566\n",
      "    432   179   473  1300    10   804    13     5  1106   318   483   566\n",
      "    432   179   473  1300    10   804    13     5  1106   318   483   566\n",
      "    432   179   473  1300    10   804    13     5  1106   318   666   318\n",
      "    707   566   432   179   666   318   707   566   432   179   666   318\n",
      "    707   566   432   179   666   318   707   566   432   179   666   318\n",
      "    707   566   432   179   666   318   707   566   432   179   666   318\n",
      "    707   566   432   179   483   566   432   179   473  1300    10   804\n",
      "     13     5  1106   318]\n",
      " [  131   306  1768  5744    11   796   145   306    13   841   254  1330\n",
      "     42  7016    54    66    17    23  5744 45415   568   548   198   154\n",
      "    349   275     2   254   284   708   314   135   135   363   126  2365\n",
      "    230   131   306  1768   545   254    11    24     5   924   726     8\n",
      "      7     1  2804  2876   134     2   521     5  1392     3  1768    23\n",
      "      1   306   355  2365   230   131   306    13     1   180    11  1101\n",
      "    133    12    68    15     5 67592  6650     9    80   706    29   306\n",
      "     71   137   145     2    61  3064     8    10    20     9    17   218\n",
      "    982  2347   167  5744]\n",
      " [    6    10  3535   203    57    98   206     9     6    10  3535   203\n",
      "     57    98   206     9     6    10  3535   203    57    98   206     9\n",
      "      6    10  3535   203    57    98   206     9     6    10  3535   203\n",
      "     57    98   206     9     6    10  3535   203    57    98   206     9\n",
      "      6    10  3535   203    57    98   206     9     6    10  3535   203\n",
      "     57    98   206     9     6    10  3535   203    57    98   206     9\n",
      "      6    10  3535   203    57    98   206     9     6    10  3535   203\n",
      "     57    98   206     9     6    10  3535 19103   239   160   106   558\n",
      "  16191 21265  2171 54443]\n",
      " [   14  1116  7963  1055 10524    14   606     9  2202     4    14   101\n",
      "   3261     2     7     6  1794  2856     7  2570    10     9     6   246\n",
      "     16    32   248  3095     2  1056  1238   976     4 54447     3     1\n",
      "    858   723    21    17    44  2564     6     1   505   742    35   335\n",
      "   1144     4  3225     2    14  1009   920   606 10694     4   587     2\n",
      "    295    14   715     4  1561   512  1276   435   359    23  5340 15651\n",
      "    350     7     8   241   240     3    32  1702   158   418    45    28\n",
      "    348     2    10    62   112   152    14   606 10694   983 14689 39541\n",
      "  22648   255   219  2555]]\n",
      "The average length of the text data is 100.00\n",
      "The maximum length of the text data is 100\n"
     ]
    }
   ],
   "source": [
    "# Limit token length to reduce the number of words in a single email\n",
    "max_length:int = 100\n",
    "X_train_padded:np.ndarray = pad_sequences(X_train_encoded, maxlen = max_length, padding = \"post\")\n",
    "print(X_train_padded[:5])\n",
    "\n",
    "# Simply measure the average length and maximum length of the text data to be used for training (X_train_padded)\n",
    "total_words:int = [len(text) for text in X_train_padded]\n",
    "max_length:int = np.max(total_words)\n",
    "print(f\"The average length of the text data is {np.mean(total_words):.2f}\")\n",
    "print(f\"The maximum length of the text data is {np.max(total_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the LSTM model\n",
    "# The vocabulary size is reduced to 1000 to reduce the number of words in the dataset\n",
    "lstm_model = train_evaluate_lstm_model(X_train, y_train, max_length=max_length, reduced_vocabulary_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
